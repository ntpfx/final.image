{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\GITHUB\\final.image/camera_params/\n",
      "[[726.47630219   0.         466.25758541]\n",
      " [  0.         732.0640605  251.997867  ]\n",
      " [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "params_dir = os.getcwd()+'/camera_params/' #Calibration -> Parallel image \n",
    "print(params_dir)\n",
    "# Load camera parameters\n",
    "ret = np.load(params_dir+'ret.npy')\n",
    "K = np.load(params_dir+'K.npy')\n",
    "dist = np.load(params_dir+'dist.npy')\n",
    "focal_length = (K[0,0] + K[1,1])/2\n",
    "\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare stereo matching variables\n",
    "#This section student has to do fine tunes by yourself for max_disp\n",
    "win_size = 5\n",
    "min_disparity = -1\n",
    "max_disparity = 63\n",
    "num_disparity = max_disparity - min_disparity #Must be divisible by 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling_image(image, reduction_scale) : # Reduce image resolution Pyr lelel\n",
    "    for i in range(0, reduction_scale) :\n",
    "        if len(image.shape) > 2:\n",
    "            row, col = image.shape[:2]\n",
    "        else :\n",
    "            row, col = image.shape\n",
    "        \n",
    "        image = cv2.pyrDown(image,dstsize=(col//2,row//2))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessimg(img):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return (img, img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = cv2.imread(\"./images/template/Template-4.png\")\n",
    "template_img,template_gray = preprocessimg(template)\n",
    "# cv2.imshow(\"AAA\",template_gray)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'release'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39melse\u001b[39;00m :\n\u001b[0;32m     35\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m left_vid\u001b[39m.\u001b[39;49mrelease()\n\u001b[0;32m     37\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'release'"
     ]
    }
   ],
   "source": [
    "left_cap = cv2.VideoCapture('./video/final_exam/Dataset-1/left_output-1.avi')\n",
    "right_cap = cv2.VideoCapture('./video/final_exam/Dataset-1/right_output-1.avi')\n",
    "\n",
    "ret ,left_vid = left_cap.read() # You have to read left and right together for syncing the video frame\n",
    "ret ,right_vid = right_cap.read() \n",
    "\n",
    "h, w = left_vid.shape[:2]\n",
    "\n",
    "#Get the new optimal camera matrix for better undistortion\n",
    "new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(K, dist, (w,h),1, (w,h))\n",
    "\n",
    "while left_cap.isOpened() and right_cap.isOpened() :\n",
    "    ret_left ,left_vid = left_cap.read() # You have to read left and right together for syncing the video frame\n",
    "    ret_right ,right_vid = right_cap.read() \n",
    "    \n",
    "    if ret_left and ret_right :\n",
    "        left_vid = cv2.cvtColor(left_vid,cv2.COLOR_BGR2GRAY)\n",
    "        right_vid = cv2.cvtColor(right_vid,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #Undistort image\n",
    "        left_vid_undistorted = cv2.undistort(left_vid, K, dist, None, new_camera_matrix)\n",
    "        right_vid_undistorted = cv2.undistort(right_vid, K, dist, None, new_camera_matrix)\n",
    "\n",
    "        #Downsampling each image 1 pyramid level in order to improve computational speed\n",
    "        left_vid_down = downsampling_image(left_vid_undistorted,1)\n",
    "        right_vi_down = downsampling_image(right_vid_undistorted,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow('Video frame', right_vi_down)\n",
    "        if cv2.waitKey(33) & 0xFF == ord('q') : # this line control the period between image frame\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "left_cap.release()\n",
    "right_cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59c2cff008705844545e81e13985dd53ac344a221d3095bc517ee3b75bd88638"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
